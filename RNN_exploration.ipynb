{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RNN_exploration.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/colindbrown/methylation-hmm-rnn/blob/master/RNN_exploration.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GeFzMcwP-2lZ",
        "colab_type": "code",
        "outputId": "a4980a88-3ab9-4687-a04f-6bbe34df421d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# imports\n",
        "import torch.nn as nn\n",
        "import torch \n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import drive\n",
        "import timeit\n",
        "\n",
        "np.set_printoptions(2)\n",
        "\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pdJLEhipKPXY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load data\n",
        "scBSseq = np.load(\"/content/drive/My Drive/Colab Notebooks/methylation_hmm_rnn/scBSseq.npz\")\n",
        "\n",
        "meth_mat = scBSseq[\"meth_mat\"] # positions x cells matrix of counts of methylated CpGs\n",
        "unmeth_mat = scBSseq[\"unmeth_mat\"] # positions x cells matrix of counts of unmethylated CpGs (i.e. C got converted to T)\n",
        "positions = scBSseq[\"positions\"] # genomic position of each CpG\n",
        "chroms = scBSseq[\"chroms\"] # chromomsome of each CpG\n",
        "cell_names = scBSseq[\"cell_names\"] # cell type and an arbitrary ID\n",
        "\n",
        "total_counts = meth_mat + unmeth_mat\n",
        "ncells_with_reads = (total_counts > 0).sum(1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LPYLacM5KWBt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# setup data loaders for training, validation, and testing\n",
        "class CpgDataset(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, meth_counts, unmeth_counts, seq_len = 1000):\n",
        "        super(CpgDataset, self).__init__()\n",
        "        self.meth_counts = meth_counts\n",
        "        self.unmeth_counts = unmeth_counts\n",
        "        self.seq_len = seq_len\n",
        "\n",
        "    def __getitem__(self, i): \n",
        "        return(self.meth_counts[i*self.seq_len:(i+1)*self.seq_len].astype(np.float32), \n",
        "               self.unmeth_counts[i*self.seq_len:(i+1)*self.seq_len].astype(np.float32))\n",
        "            \n",
        "    def __len__(self):\n",
        "        return self.meth_counts.shape[0] // self.seq_len\n",
        "\n",
        "batch_size = 100\n",
        "seq_len = 1000\n",
        "\n",
        "hidden_size = 32\n",
        "\n",
        "training_cpg = chroms == \"18\"\n",
        "train_dataset = CpgDataset(meth_mat[training_cpg,:], unmeth_mat[training_cpg,:], seq_len = seq_len)\n",
        "train_dataloader = torch.utils.data.DataLoader(train_dataset, \n",
        "                                               batch_size = batch_size, \n",
        "                                               num_workers = 0 )\n",
        "val_cpg = chroms == \"19\"\n",
        "val_dataset = CpgDataset(meth_mat[val_cpg,:], unmeth_mat[val_cpg,:], seq_len = seq_len)\n",
        "validation_dataloader = torch.utils.data.DataLoader(val_dataset, \n",
        "                                               batch_size = batch_size, \n",
        "                                               num_workers = 0)\n",
        "test_cpg = chroms == \"17\"\n",
        "test_dataset = CpgDataset(meth_mat[test_cpg,:], unmeth_mat[test_cpg,:], seq_len = seq_len)\n",
        "test_dataloader = torch.utils.data.DataLoader(test_dataset, \n",
        "                                               batch_size = batch_size, \n",
        "                                               num_workers = 0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z5bm2EfUISYI",
        "colab_type": "text"
      },
      "source": [
        "# Implementing a RNN with Memory\n",
        "Variant on a basic RNN cell that mimics some of the behavior of a GRU, where\n",
        "$$ f_t = \\sigma( W_f [ x_t , h_{t-1} ]^T + \\mu_f ) $$\n",
        "$$ g_t = \\tanh( W_g [ x_t , h_{t-1} ]^T + \\mu_g ) $$\n",
        "$$ h_t = f_t \\cdot h_{t-1} + (1 - f_t) \\cdot g_t $$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dr6JWg64HwgK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Implementation of an individual RNN Cell with basic memory\n",
        "class RNNCell(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(RNNCell, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.linearF = nn.Linear(input_size + hidden_size, hidden_size)\n",
        "        self.linearG = nn.Linear(input_size + hidden_size, hidden_size)\n",
        "        \n",
        "    def forward(self, x, h_in):\n",
        "        \"\"\"\n",
        "        Forward computation\n",
        "        \n",
        "        Parameters:\n",
        "        x     - input\n",
        "                 torch.tensor [batch_size x input_size] \n",
        "        h_in  - hidden state from previous time point\n",
        "                 torch.tensor [batch_size x hidden_size]\n",
        "            \n",
        "        Returns:\n",
        "        h_out - new hidden state \n",
        "                  torch.tensor [batch_size x hidden_size]\n",
        "        \"\"\"\n",
        "        combined = torch.cat((x, h_in), 1)\n",
        "        f = torch.sigmoid( self.linearF(combined) )\n",
        "        g = torch.tanh( self.linearG(combined) )\n",
        "        h_out = f * h_in + (1-f) * g\n",
        "        return h_out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FIu5k8V3Jqfx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Implementation of a full RNN Layer\n",
        "class RNNLayer(nn.Module):\n",
        "    def __init__(self, rnn_cell):\n",
        "        super(RNNLayer, self).__init__()\n",
        "        self.rnn_cell = rnn_cell\n",
        "\n",
        "    @property\n",
        "    def hidden_size(self):\n",
        "        return self.rnn_cell.hidden_size\n",
        "\n",
        "    @property\n",
        "    def input_size(self):\n",
        "        return self.rnn_cell.input_size\n",
        "        \n",
        "    def forward(self, x): \n",
        "        (seq_len, batch_size, input_size) = x.shape\n",
        "        hidden = torch.zeros(batch_size, self.hidden_size, device = x.device) # could be an input to the function\n",
        "        output = torch.zeros(seq_len, batch_size, self.hidden_size, device = x.device)\n",
        "        for i in range(seq_len): # iterate over sequence\n",
        "            hidden = self.rnn_cell(x[i,:,:], hidden)\n",
        "            output[i,:,:] = hidden\n",
        "        return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "clG_x3P_H1Y_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Model to predict methylation of next position from preceding positions\n",
        "class MethModel(nn.Module):\n",
        "    def __init__(self, rnn_layer, input_size = 2, output_size = 1):\n",
        "        super(MethModel, self).__init__()\n",
        "        self.hidden_size = rnn_layer.hidden_size\n",
        "        self.embed = nn.Linear(input_size, rnn_layer.input_size)\n",
        "        self.rnn_layer = rnn_layer\n",
        "        self.linear = nn.Linear(self.hidden_size, output_size)\n",
        "\n",
        "    def forward(self, meth, unmeth):\n",
        "        batch_size = meth.shape[1]\n",
        "        num_cells = meth.shape[2]\n",
        "        meth = meth.view(meth.shape[0], batch_size * num_cells)\n",
        "        unmeth = unmeth.view(unmeth.shape[0], batch_size * num_cells)\n",
        "        x = torch.stack((meth,unmeth),2)\n",
        "        x = self.embed(x)\n",
        "        x = F.relu(x)\n",
        "        h = self.rnn_layer(x) \n",
        "        if isinstance(rnn_layer, nn.RNNBase): h = h[0]\n",
        "        output = self.linear(h)\n",
        "        output = output.view(output.shape[0], batch_size, num_cells, output.shape[-1])\n",
        "        output = output[:,:,:,0]\n",
        "        return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1yO1iZU1IAw8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# training methods\n",
        "def run_one_epoch(train_flag, dataloader, model, optimizer, device=\"cuda\"):\n",
        "\n",
        "    torch.set_grad_enabled(train_flag)\n",
        "    model.train() if train_flag else model.eval()\n",
        "\n",
        "    losses = []\n",
        "    accuracies = []\n",
        "\n",
        "    for meth,unmeth in dataloader:\n",
        "        meth = meth.transpose(0,1).contiguous() \n",
        "        unmeth = unmeth.transpose(0,1).contiguous()\n",
        "\n",
        "        (meth, unmeth) = ( meth.to(device), unmeth.to(device) )\n",
        "\n",
        "        # forward\n",
        "        logodds = model(meth, unmeth)\n",
        "\n",
        "        lo = logodds[:-1,:,:] # shift by one since we're doing one-step ahead prediction\n",
        "        prob_methylated = torch.sigmoid(lo)\n",
        "\n",
        "        logProb = torch.log(prob_methylated)\n",
        "        logCompProb = torch.log(1-prob_methylated)\n",
        "        loss = - torch.mean(logProb*meth[1:,:,:] + logCompProb*unmeth[1:,:,:])\n",
        "        \n",
        "        losses.append(loss.detach().cpu().numpy())\n",
        "\n",
        "        if train_flag: \n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        # calculate accuracy\n",
        "        meth_sum = meth[1:,:,:] + unmeth[1:,:,:]\n",
        "        meth_rate = meth[1:,:,:] / meth_sum\n",
        "        correct = ((meth_rate > .5) == (prob_methylated > .5))[meth_sum > 0]\n",
        "        accuracy = torch.mean( correct.float() )\n",
        "\n",
        "        accuracies.append(accuracy.detach().cpu().numpy())  \n",
        "\n",
        "    return( np.mean(losses), np.mean(accuracies) )\n",
        "\n",
        "def train_loop(model,\n",
        "               optimizer,\n",
        "               train_dataloader, \n",
        "               validation_dataloader,\n",
        "               check_point_filename = '/content/drive/My Drive/scBSseq/rnn_checkpoint.pt',\n",
        "               max_epochs = 100, \n",
        "               patience = 10,\n",
        "               device = \"cuda\"):\n",
        "\n",
        "    train_accs = []\n",
        "    val_accs = []\n",
        "    patience_counter = patience\n",
        "    best_val_loss = np.inf\n",
        "\n",
        "    for epoch in range(max_epochs):\n",
        "        start_time = timeit.default_timer()\n",
        "        train_loss, train_acc = run_one_epoch(True, train_dataloader, model, optimizer, device)\n",
        "        val_loss, val_acc = run_one_epoch(False, validation_dataloader, model, optimizer, device)\n",
        "        train_accs.append(train_acc)\n",
        "        val_accs.append(val_acc)\n",
        "\n",
        "        elapsed = float(timeit.default_timer() - start_time)\n",
        "        print(\"Epoch %i took %.2fs. Train loss: %.4f acc: %.4f. Val loss: %.4f acc: %.4f. Patience left: %i\" % \n",
        "            (epoch+1, elapsed, train_loss, train_acc, val_loss, val_acc, patience_counter ))\n",
        "        \n",
        "        if val_loss < best_val_loss: \n",
        "            torch.save(model.state_dict(), check_point_filename)\n",
        "            best_val_loss = val_loss\n",
        "            patience_counter = patience\n",
        "        else: \n",
        "            patience_counter -= 1\n",
        "            if patience_counter <= 0: \n",
        "                model.load_state_dict(torch.load(check_point_filename)) # recover the best model so far\n",
        "                break\n",
        "    \n",
        "    return(train_accs, val_accs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G0zZretWIE20",
        "colab_type": "code",
        "outputId": "8bbe8c99-a780-4700-991e-04c7a49c0b05",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 544
        }
      },
      "source": [
        "# Train RNN and print accuracy\n",
        "rnn_cell = RNNCell(input_size = hidden_size, hidden_size = hidden_size) \n",
        "rnn_layer = RNNLayer(rnn_cell)\n",
        "\n",
        "model = MethModel(rnn_layer)\n",
        "\n",
        "device = \"cuda\"\n",
        "model.to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), amsgrad=True)\n",
        "\n",
        "train_accs, val_accs = train_loop(model,\n",
        "                                optimizer,\n",
        "                                train_dataloader, \n",
        "                                validation_dataloader,\n",
        "                                check_point_filename = 'rnn_checkpoint.pt',\n",
        "                                device = device,\n",
        "                                max_epochs = 30)\n",
        "\n",
        "test_loss, test_acc = run_one_epoch(False, test_dataloader, model, optimizer, device)\n",
        "print(test_acc)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 took 10.98s. Train loss: 0.3924 acc: 0.3723. Val loss: 0.4030 acc: 0.3791. Patience left: 10\n",
            "Epoch 2 took 10.97s. Train loss: 0.3805 acc: 0.4246. Val loss: 0.3897 acc: 0.4700. Patience left: 10\n",
            "Epoch 3 took 10.99s. Train loss: 0.3695 acc: 0.5112. Val loss: 0.3772 acc: 0.5741. Patience left: 10\n",
            "Epoch 4 took 10.97s. Train loss: 0.3591 acc: 0.6122. Val loss: 0.3654 acc: 0.6912. Patience left: 10\n",
            "Epoch 5 took 11.03s. Train loss: 0.3493 acc: 0.7035. Val loss: 0.3541 acc: 0.7863. Patience left: 10\n",
            "Epoch 6 took 10.98s. Train loss: 0.3401 acc: 0.7700. Val loss: 0.3434 acc: 0.7549. Patience left: 10\n",
            "Epoch 7 took 10.96s. Train loss: 0.3313 acc: 0.7016. Val loss: 0.3332 acc: 0.7024. Patience left: 10\n",
            "Epoch 8 took 10.96s. Train loss: 0.3230 acc: 0.6635. Val loss: 0.3235 acc: 0.6927. Patience left: 10\n",
            "Epoch 9 took 11.00s. Train loss: 0.3152 acc: 0.6567. Val loss: 0.3143 acc: 0.6913. Patience left: 10\n",
            "Epoch 10 took 10.97s. Train loss: 0.3078 acc: 0.6566. Val loss: 0.3056 acc: 0.6932. Patience left: 10\n",
            "Epoch 11 took 10.94s. Train loss: 0.3008 acc: 0.6594. Val loss: 0.2973 acc: 0.6979. Patience left: 10\n",
            "Epoch 12 took 11.01s. Train loss: 0.2939 acc: 0.6652. Val loss: 0.2893 acc: 0.7054. Patience left: 10\n",
            "Epoch 13 took 10.99s. Train loss: 0.2870 acc: 0.6744. Val loss: 0.2811 acc: 0.7168. Patience left: 10\n",
            "Epoch 14 took 10.97s. Train loss: 0.2797 acc: 0.6880. Val loss: 0.2727 acc: 0.7335. Patience left: 10\n",
            "Epoch 15 took 10.97s. Train loss: 0.2718 acc: 0.7080. Val loss: 0.2638 acc: 0.7559. Patience left: 10\n",
            "Epoch 16 took 11.00s. Train loss: 0.2630 acc: 0.7350. Val loss: 0.2542 acc: 0.7842. Patience left: 10\n",
            "Epoch 17 took 10.99s. Train loss: 0.2533 acc: 0.7692. Val loss: 0.2442 acc: 0.8141. Patience left: 10\n",
            "Epoch 18 took 10.97s. Train loss: 0.2430 acc: 0.8034. Val loss: 0.2344 acc: 0.8365. Patience left: 10\n",
            "Epoch 19 took 11.00s. Train loss: 0.2330 acc: 0.8238. Val loss: 0.2258 acc: 0.8330. Patience left: 10\n",
            "Epoch 20 took 11.04s. Train loss: 0.2247 acc: 0.8176. Val loss: 0.2189 acc: 0.8238. Patience left: 10\n",
            "Epoch 21 took 11.00s. Train loss: 0.2183 acc: 0.8118. Val loss: 0.2141 acc: 0.8204. Patience left: 10\n",
            "Epoch 22 took 10.99s. Train loss: 0.2162 acc: 0.8075. Val loss: 0.2167 acc: 0.8133. Patience left: 10\n",
            "Epoch 23 took 10.98s. Train loss: 0.2190 acc: 0.8026. Val loss: 0.2166 acc: 0.8133. Patience left: 9\n",
            "Epoch 24 took 11.01s. Train loss: 0.2180 acc: 0.8042. Val loss: 0.2120 acc: 0.8195. Patience left: 8\n",
            "Epoch 25 took 11.01s. Train loss: 0.2133 acc: 0.8103. Val loss: 0.2078 acc: 0.8269. Patience left: 10\n",
            "Epoch 26 took 10.96s. Train loss: 0.2101 acc: 0.8164. Val loss: 0.2070 acc: 0.8320. Patience left: 10\n",
            "Epoch 27 took 11.00s. Train loss: 0.2099 acc: 0.8196. Val loss: 0.2068 acc: 0.8352. Patience left: 10\n",
            "Epoch 28 took 11.02s. Train loss: 0.2098 acc: 0.8223. Val loss: 0.2059 acc: 0.8370. Patience left: 10\n",
            "Epoch 29 took 10.98s. Train loss: 0.2092 acc: 0.8229. Val loss: 0.2051 acc: 0.8357. Patience left: 10\n",
            "Epoch 30 took 10.96s. Train loss: 0.2087 acc: 0.8215. Val loss: 0.2042 acc: 0.8345. Patience left: 10\n",
            "0.82518154\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IflfgdxzINzx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.plot(train_accs,  \"--\", label=\"train\")\n",
        "plt.plot(val_accs, label=\"validation\")\n",
        "plt.axhline(test_acc)\n",
        "plt.legend()\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.ylabel(\"accuracy\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dH6ZYrG2IccG",
        "colab_type": "text"
      },
      "source": [
        "# Comparing RNN Variants\n",
        "Comparison of MyRNN implementation to PyTorch's built in RNN, GRU, and LSTM layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_wS4D0e2ImrZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Different model class for torch implementations, as torch layers return a tuple output\n",
        "class TorchMethModel(nn.Module):\n",
        "    def __init__(self, rnn_layer, input_size = 2, output_size = 1):\n",
        "        super(TorchMethModel, self).__init__()\n",
        "        self.hidden_size = rnn_layer.hidden_size\n",
        "        self.embed = nn.Linear(input_size, rnn_layer.input_size)\n",
        "        self.rnn_layer = rnn_layer\n",
        "        self.linear = nn.Linear(self.hidden_size, output_size)\n",
        "\n",
        "    def forward(self, meth, unmeth):\n",
        "        batch_size = meth.shape[1]\n",
        "        num_cells = meth.shape[2]\n",
        "        meth = meth.view(meth.shape[0], batch_size * num_cells)\n",
        "        unmeth = unmeth.view(unmeth.shape[0], batch_size * num_cells)\n",
        "        x = torch.stack((meth,unmeth),2)\n",
        "        x = self.embed(x)\n",
        "        x = F.relu(x)\n",
        "        # handles extra output from torch implementation\n",
        "        h, _ = self.rnn_layer(x) \n",
        "        if isinstance(rnn_layer, nn.RNNBase): h = h[0]\n",
        "        output = self.linear(h)\n",
        "        output = output.view(output.shape[0], batch_size, num_cells, output.shape[-1])\n",
        "        output = output[:,:,:,0]\n",
        "        return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qeN3B8IIIqtn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Training wrapper that runs for multiple provided models\n",
        "# Plots validation accuracy across training for each\n",
        "def train_and_plot(models, type, accs):\n",
        "  for restart, model in enumerate(models):\n",
        "    print(f'{type} Restart {restart}')\n",
        "\n",
        "    device = \"cuda\"\n",
        "    model.to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), amsgrad=True)\n",
        "\n",
        "    train_accs, val_accs = train_loop(model,\n",
        "                                optimizer,\n",
        "                                train_dataloader, \n",
        "                                validation_dataloader,\n",
        "                                check_point_filename = f'{type}_checkpoint_{restart}.pt',\n",
        "                                device = device,\n",
        "                                max_epochs = 30)\n",
        "\n",
        "    test_loss, test_acc = run_one_epoch(False, test_dataloader, model, optimizer, device)\n",
        "    accs[\"train\"].append(train_accs[-1])\n",
        "    accs[\"val\"].append(val_accs[-1])\n",
        "    accs[\"test\"].append(test_acc)\n",
        "    print(test_acc)\n",
        "\n",
        "    plt.plot(val_accs, label=f\"val accs {restart}\")\n",
        "\n",
        "  plt.legend()\n",
        "  plt.ylabel('Accuracy')\n",
        "  plt.title(\"Validation Accuracy over Training\")\n",
        "  plt.grid(which=\"both\")\n",
        "  plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4rxz1y2BItTj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Training using MyRNN implementation\n",
        "MyRNN_accs = {\"train\": [], \"val\": [], \"test\": []}\n",
        "MyRNN_models = []\n",
        "for i in range(3):\n",
        "  rnn_cell = RNNCell(input_size = hidden_size, hidden_size = hidden_size) \n",
        "  rnn_layer = RNNLayer(rnn_cell)\n",
        "  MyRNN_models.append(MethModel(rnn_layer))\n",
        "train_and_plot(MyRNN_models, \"MyRNN\", MyRNN_accs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LLcN1NVcIwV-",
        "colab_type": "code",
        "outputId": "be268448-a28e-4c25-9c33-6699f960808e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Training using PyTorch RNN implementation\n",
        "RNN_accs = {\"train\": [], \"val\": [], \"test\": []}\n",
        "RNN_models = []\n",
        "for i in range(3):\n",
        "  layer = nn.RNN(input_size=hidden_size, hidden_size=hidden_size)\n",
        "  RNN_models.append(TorchMethModel(layer))\n",
        "train_and_plot(RNN_models, \"RNN\", RNN_accs)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RNN Restart 0\n",
            "Epoch 1 took 0.53s. Train loss: 0.3708 acc: 0.6414. Val loss: 0.3738 acc: 0.6764. Patience left: 10\n",
            "Epoch 2 took 0.51s. Train loss: 0.3593 acc: 0.6415. Val loss: 0.3612 acc: 0.6765. Patience left: 10\n",
            "Epoch 3 took 0.51s. Train loss: 0.3491 acc: 0.6416. Val loss: 0.3500 acc: 0.6766. Patience left: 10\n",
            "Epoch 4 took 0.50s. Train loss: 0.3402 acc: 0.6418. Val loss: 0.3402 acc: 0.6769. Patience left: 10\n",
            "Epoch 5 took 0.50s. Train loss: 0.3324 acc: 0.6421. Val loss: 0.3315 acc: 0.6774. Patience left: 10\n",
            "Epoch 6 took 0.50s. Train loss: 0.3255 acc: 0.6427. Val loss: 0.3238 acc: 0.6780. Patience left: 10\n",
            "Epoch 7 took 0.50s. Train loss: 0.3192 acc: 0.6436. Val loss: 0.3168 acc: 0.6794. Patience left: 10\n",
            "Epoch 8 took 0.51s. Train loss: 0.3134 acc: 0.6451. Val loss: 0.3102 acc: 0.6807. Patience left: 10\n",
            "Epoch 9 took 0.49s. Train loss: 0.3079 acc: 0.6475. Val loss: 0.3039 acc: 0.6843. Patience left: 10\n",
            "Epoch 10 took 0.49s. Train loss: 0.3024 acc: 0.6504. Val loss: 0.2978 acc: 0.6872. Patience left: 10\n",
            "Epoch 11 took 0.50s. Train loss: 0.2968 acc: 0.6559. Val loss: 0.2916 acc: 0.6942. Patience left: 10\n",
            "Epoch 12 took 0.50s. Train loss: 0.2912 acc: 0.6622. Val loss: 0.2854 acc: 0.6997. Patience left: 10\n",
            "Epoch 13 took 0.50s. Train loss: 0.2853 acc: 0.6703. Val loss: 0.2791 acc: 0.7108. Patience left: 10\n",
            "Epoch 14 took 0.50s. Train loss: 0.2792 acc: 0.6822. Val loss: 0.2727 acc: 0.7222. Patience left: 10\n",
            "Epoch 15 took 0.50s. Train loss: 0.2730 acc: 0.6956. Val loss: 0.2662 acc: 0.7337. Patience left: 10\n",
            "Epoch 16 took 0.50s. Train loss: 0.2666 acc: 0.7103. Val loss: 0.2595 acc: 0.7515. Patience left: 10\n",
            "Epoch 17 took 0.51s. Train loss: 0.2600 acc: 0.7325. Val loss: 0.2528 acc: 0.7754. Patience left: 10\n",
            "Epoch 18 took 0.49s. Train loss: 0.2533 acc: 0.7574. Val loss: 0.2459 acc: 0.8002. Patience left: 10\n",
            "Epoch 19 took 0.50s. Train loss: 0.2466 acc: 0.7803. Val loss: 0.2389 acc: 0.8134. Patience left: 10\n",
            "Epoch 20 took 0.50s. Train loss: 0.2398 acc: 0.7958. Val loss: 0.2318 acc: 0.8285. Patience left: 10\n",
            "Epoch 21 took 0.50s. Train loss: 0.2329 acc: 0.8161. Val loss: 0.2245 acc: 0.8325. Patience left: 10\n",
            "Epoch 22 took 0.50s. Train loss: 0.2260 acc: 0.8177. Val loss: 0.2173 acc: 0.8339. Patience left: 10\n",
            "Epoch 23 took 0.49s. Train loss: 0.2193 acc: 0.8201. Val loss: 0.2105 acc: 0.8370. Patience left: 10\n",
            "Epoch 24 took 0.49s. Train loss: 0.2133 acc: 0.8228. Val loss: 0.2068 acc: 0.8362. Patience left: 10\n",
            "Epoch 25 took 0.49s. Train loss: 0.2110 acc: 0.8223. Val loss: 0.2061 acc: 0.8332. Patience left: 10\n",
            "Epoch 26 took 0.49s. Train loss: 0.2099 acc: 0.8200. Val loss: 0.2054 acc: 0.8284. Patience left: 10\n",
            "Epoch 27 took 0.49s. Train loss: 0.2096 acc: 0.8144. Val loss: 0.2064 acc: 0.8230. Patience left: 10\n",
            "Epoch 28 took 0.49s. Train loss: 0.2105 acc: 0.8095. Val loss: 0.2057 acc: 0.8256. Patience left: 9\n",
            "Epoch 29 took 0.49s. Train loss: 0.2098 acc: 0.8123. Val loss: 0.2041 acc: 0.8278. Patience left: 8\n",
            "Epoch 30 took 0.49s. Train loss: 0.2082 acc: 0.8150. Val loss: 0.2023 acc: 0.8332. Patience left: 10\n",
            "0.82420427\n",
            "RNN Restart 1\n",
            "Epoch 1 took 0.50s. Train loss: 0.3690 acc: 0.6576. Val loss: 0.3706 acc: 0.6961. Patience left: 10\n",
            "Epoch 2 took 0.50s. Train loss: 0.3548 acc: 0.6635. Val loss: 0.3551 acc: 0.6969. Patience left: 10\n",
            "Epoch 3 took 0.50s. Train loss: 0.3420 acc: 0.6641. Val loss: 0.3412 acc: 0.6972. Patience left: 10\n",
            "Epoch 4 took 0.49s. Train loss: 0.3304 acc: 0.6646. Val loss: 0.3286 acc: 0.6978. Patience left: 10\n",
            "Epoch 5 took 0.51s. Train loss: 0.3199 acc: 0.6654. Val loss: 0.3172 acc: 0.6996. Patience left: 10\n",
            "Epoch 6 took 0.49s. Train loss: 0.3103 acc: 0.6681. Val loss: 0.3068 acc: 0.7029. Patience left: 10\n",
            "Epoch 7 took 0.50s. Train loss: 0.3015 acc: 0.6747. Val loss: 0.2971 acc: 0.7157. Patience left: 10\n",
            "Epoch 8 took 0.49s. Train loss: 0.2932 acc: 0.6875. Val loss: 0.2879 acc: 0.7204. Patience left: 10\n",
            "Epoch 9 took 0.49s. Train loss: 0.2851 acc: 0.6919. Val loss: 0.2790 acc: 0.7239. Patience left: 10\n",
            "Epoch 10 took 0.49s. Train loss: 0.2771 acc: 0.6963. Val loss: 0.2703 acc: 0.7306. Patience left: 10\n",
            "Epoch 11 took 0.50s. Train loss: 0.2691 acc: 0.7044. Val loss: 0.2616 acc: 0.7450. Patience left: 10\n",
            "Epoch 12 took 0.50s. Train loss: 0.2609 acc: 0.7240. Val loss: 0.2528 acc: 0.7745. Patience left: 10\n",
            "Epoch 13 took 0.50s. Train loss: 0.2525 acc: 0.7598. Val loss: 0.2440 acc: 0.8044. Patience left: 10\n",
            "Epoch 14 took 0.50s. Train loss: 0.2440 acc: 0.7916. Val loss: 0.2353 acc: 0.8305. Patience left: 10\n",
            "Epoch 15 took 0.50s. Train loss: 0.2357 acc: 0.8153. Val loss: 0.2269 acc: 0.8307. Patience left: 10\n",
            "Epoch 16 took 0.50s. Train loss: 0.2277 acc: 0.8164. Val loss: 0.2190 acc: 0.8291. Patience left: 10\n",
            "Epoch 17 took 0.50s. Train loss: 0.2202 acc: 0.8155. Val loss: 0.2121 acc: 0.8296. Patience left: 10\n",
            "Epoch 18 took 0.49s. Train loss: 0.2145 acc: 0.8171. Val loss: 0.2112 acc: 0.8278. Patience left: 10\n",
            "Epoch 19 took 0.50s. Train loss: 0.2147 acc: 0.8148. Val loss: 0.2104 acc: 0.8248. Patience left: 10\n",
            "Epoch 20 took 0.50s. Train loss: 0.2130 acc: 0.8123. Val loss: 0.2099 acc: 0.8180. Patience left: 10\n",
            "Epoch 21 took 0.50s. Train loss: 0.2138 acc: 0.8039. Val loss: 0.2096 acc: 0.8174. Patience left: 10\n",
            "Epoch 22 took 0.50s. Train loss: 0.2124 acc: 0.8070. Val loss: 0.2068 acc: 0.8265. Patience left: 10\n",
            "Epoch 23 took 0.49s. Train loss: 0.2106 acc: 0.8151. Val loss: 0.2057 acc: 0.8296. Patience left: 10\n",
            "Epoch 24 took 0.49s. Train loss: 0.2094 acc: 0.8181. Val loss: 0.2030 acc: 0.8339. Patience left: 10\n",
            "Epoch 25 took 0.49s. Train loss: 0.2074 acc: 0.8206. Val loss: 0.2022 acc: 0.8344. Patience left: 10\n",
            "Epoch 26 took 0.49s. Train loss: 0.2069 acc: 0.8208. Val loss: 0.2020 acc: 0.8365. Patience left: 10\n",
            "Epoch 27 took 0.50s. Train loss: 0.2068 acc: 0.8228. Val loss: 0.2018 acc: 0.8378. Patience left: 10\n",
            "Epoch 28 took 0.49s. Train loss: 0.2067 acc: 0.8241. Val loss: 0.2015 acc: 0.8390. Patience left: 10\n",
            "Epoch 29 took 0.50s. Train loss: 0.2065 acc: 0.8251. Val loss: 0.2010 acc: 0.8394. Patience left: 10\n",
            "Epoch 30 took 0.49s. Train loss: 0.2060 acc: 0.8254. Val loss: 0.2005 acc: 0.8394. Patience left: 10\n",
            "0.82951945\n",
            "RNN Restart 2\n",
            "Epoch 1 took 0.50s. Train loss: 0.3371 acc: 0.6418. Val loss: 0.3368 acc: 0.6770. Patience left: 10\n",
            "Epoch 2 took 0.49s. Train loss: 0.3276 acc: 0.6422. Val loss: 0.3263 acc: 0.6774. Patience left: 10\n",
            "Epoch 3 took 0.50s. Train loss: 0.3190 acc: 0.6429. Val loss: 0.3166 acc: 0.6787. Patience left: 10\n",
            "Epoch 4 took 0.50s. Train loss: 0.3109 acc: 0.6444. Val loss: 0.3078 acc: 0.6805. Patience left: 10\n",
            "Epoch 5 took 0.50s. Train loss: 0.3034 acc: 0.6478. Val loss: 0.2995 acc: 0.6855. Patience left: 10\n",
            "Epoch 6 took 0.49s. Train loss: 0.2964 acc: 0.6531. Val loss: 0.2919 acc: 0.6911. Patience left: 10\n",
            "Epoch 7 took 0.49s. Train loss: 0.2896 acc: 0.6638. Val loss: 0.2846 acc: 0.7077. Patience left: 10\n",
            "Epoch 8 took 0.49s. Train loss: 0.2830 acc: 0.6815. Val loss: 0.2776 acc: 0.7172. Patience left: 10\n",
            "Epoch 9 took 0.51s. Train loss: 0.2766 acc: 0.6885. Val loss: 0.2710 acc: 0.7515. Patience left: 10\n",
            "Epoch 10 took 0.50s. Train loss: 0.2704 acc: 0.7373. Val loss: 0.2646 acc: 0.7699. Patience left: 10\n",
            "Epoch 11 took 0.49s. Train loss: 0.2644 acc: 0.7480. Val loss: 0.2586 acc: 0.7817. Patience left: 10\n",
            "Epoch 12 took 0.49s. Train loss: 0.2587 acc: 0.7759. Val loss: 0.2529 acc: 0.8076. Patience left: 10\n",
            "Epoch 13 took 0.50s. Train loss: 0.2533 acc: 0.7916. Val loss: 0.2475 acc: 0.8046. Patience left: 10\n",
            "Epoch 14 took 0.49s. Train loss: 0.2482 acc: 0.7890. Val loss: 0.2423 acc: 0.8050. Patience left: 10\n",
            "Epoch 15 took 0.50s. Train loss: 0.2434 acc: 0.7895. Val loss: 0.2374 acc: 0.8059. Patience left: 10\n",
            "Epoch 16 took 0.49s. Train loss: 0.2389 acc: 0.7907. Val loss: 0.2327 acc: 0.8070. Patience left: 10\n",
            "Epoch 17 took 0.50s. Train loss: 0.2346 acc: 0.7921. Val loss: 0.2283 acc: 0.8087. Patience left: 10\n",
            "Epoch 18 took 0.49s. Train loss: 0.2307 acc: 0.7938. Val loss: 0.2241 acc: 0.8106. Patience left: 10\n",
            "Epoch 19 took 0.50s. Train loss: 0.2269 acc: 0.7959. Val loss: 0.2202 acc: 0.8126. Patience left: 10\n",
            "Epoch 20 took 0.50s. Train loss: 0.2235 acc: 0.7980. Val loss: 0.2168 acc: 0.8111. Patience left: 10\n",
            "Epoch 21 took 0.50s. Train loss: 0.2205 acc: 0.7962. Val loss: 0.2137 acc: 0.8138. Patience left: 10\n",
            "Epoch 22 took 0.49s. Train loss: 0.2178 acc: 0.7992. Val loss: 0.2110 acc: 0.8177. Patience left: 10\n",
            "Epoch 23 took 0.49s. Train loss: 0.2154 acc: 0.8033. Val loss: 0.2088 acc: 0.8215. Patience left: 10\n",
            "Epoch 24 took 0.49s. Train loss: 0.2134 acc: 0.8074. Val loss: 0.2070 acc: 0.8256. Patience left: 10\n",
            "Epoch 25 took 0.50s. Train loss: 0.2119 acc: 0.8116. Val loss: 0.2058 acc: 0.8291. Patience left: 10\n",
            "Epoch 26 took 0.50s. Train loss: 0.2108 acc: 0.8151. Val loss: 0.2050 acc: 0.8316. Patience left: 10\n",
            "Epoch 27 took 0.49s. Train loss: 0.2102 acc: 0.8174. Val loss: 0.2045 acc: 0.8309. Patience left: 10\n",
            "Epoch 28 took 0.50s. Train loss: 0.2096 acc: 0.8177. Val loss: 0.2039 acc: 0.8322. Patience left: 10\n",
            "Epoch 29 took 0.50s. Train loss: 0.2089 acc: 0.8192. Val loss: 0.2029 acc: 0.8340. Patience left: 10\n",
            "Epoch 30 took 0.50s. Train loss: 0.2079 acc: 0.8210. Val loss: 0.2017 acc: 0.8360. Patience left: 10\n",
            "0.82626575\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYkAAAEICAYAAACqMQjAAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eXgBtBsZIypH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Training using PyTorch GRU implementation\n",
        "GRU_accs = {\"train\": [], \"val\": [], \"test\": []}\n",
        "GRU_models = []\n",
        "for i in range(3):\n",
        "  layer = nn.GRU(input_size=hidden_size, hidden_size=hidden_size)\n",
        "  GRU_models.append(TorchMethModel(layer))\n",
        "train_and_plot(GRU_models, \"GRU\", GRU_accs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tWQAskjKI1Tb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Training using PyTorch LSTM implementation\n",
        "LSTM_accs = {\"train\": [], \"val\": [], \"test\": []}\n",
        "LSTM_models = []\n",
        "for i in range(3):\n",
        "  layer = nn.LSTM(input_size=hidden_size, hidden_size=hidden_size)\n",
        "  LSTM_models.append(TorchMethModel(layer))\n",
        "train_and_plot(LSTM_models, \"LSTM\", LSTM_accs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_MC3qHuMI33K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Compare training, validation, and test accuracy for 4 different models on same data\n",
        "labels = ['MyRNN', 'RNN', 'GRU', 'LSTM']\n",
        "accs = [MyRNN_accs, RNN_accs, GRU_accs, LSTM_accs]\n",
        "\n",
        "def plot(acc_type, name):\n",
        "  x = np.arange(len(labels))\n",
        "  width = 0.15\n",
        "\n",
        "  fig, ax = plt.subplots()\n",
        "  restart1 = ax.bar(x - width, list(map(lambda x: x[acc_type][0],accs)), width, label='Restart 1')\n",
        "  restart2 = ax.bar(x, list(map(lambda x: x[acc_type][1],accs)), width, label='Restart 2')\n",
        "  restart3 = ax.bar(x + width, list(map(lambda x: x[acc_type][2],accs)), width, label='Restart 3')\n",
        "\n",
        "  ax.set_ylabel('Accuracy')\n",
        "  plt.ylim((.80, .85))   # s\n",
        "  ax.set_title(name)\n",
        "  ax.set_xticks(x)\n",
        "  ax.set_xticklabels(labels)\n",
        "  ax.legend()\n",
        "\n",
        "  fig.tight_layout()\n",
        "  plt.show()  \n",
        "\n",
        "plot(\"train\", \"Training Accuracy for RNN Variants\")\n",
        "plot(\"val\", \"Validation Accuracy for RNN Variants\")\n",
        "plot(\"test\", \"Test Accuracy for RNN Variants\")"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}